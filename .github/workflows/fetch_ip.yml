name: Update IP and Time (BJ)

on:
  schedule:
    # GitHub 免费版限制最小间隔约 5 分钟
    - cron: '*/15 * * * *'
  workflow_dispatch:

# =========================================================
# 核心配置区域 (输出均为 .md 格式)
# =========================================================
env:
  # --- 1. IPv4 速度测试 (15分钟/历史) ---
  FILE_15M_IP_V4: "IPv4_15分钟内_纯IP.md"
  FILE_15M_INFO_V4: "IPv4_15分钟内_完整信息.md"
  FILE_HIST_IP_V4: "IPv4_历史最快_纯IP.md"
  FILE_HIST_INFO_V4: "IPv4_历史最快_完整信息.md"
  
  # --- 2. IPv6 速度测试 (15分钟/历史) ---
  FILE_15M_IP_V6: "IPv6_15分钟内_纯IP.md"
  FILE_15M_INFO_V6: "IPv6_15分钟内_完整信息.md"
  FILE_HIST_IP_V6: "IPv6_历史最快_纯IP.md"
  FILE_HIST_INFO_V6: "IPv6_历史最快_完整信息.md"
  
  # --- 3. 延迟与数据中心 ---
  FILE_LATENCY: "24小时三网延迟最低统计.md"
  FILE_COLO: "CloudFlare数据中心.md"
  
  # --- 4. 日志 ---
  FILE_LOG: "运行日志.md"

# =========================================================

permissions:
  contents: write

jobs:
  fetch-and-update:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Dependencies
        run: pip install requests beautifulsoup4

      - name: Run Script
        # 传递环境变量
        env:
          FILE_15M_IP_V4: ${{ env.FILE_15M_IP_V4 }}
          FILE_15M_INFO_V4: ${{ env.FILE_15M_INFO_V4 }}
          FILE_HIST_IP_V4: ${{ env.FILE_HIST_IP_V4 }}
          FILE_HIST_INFO_V4: ${{ env.FILE_HIST_INFO_V4 }}
          FILE_15M_IP_V6: ${{ env.FILE_15M_IP_V6 }}
          FILE_15M_INFO_V6: ${{ env.FILE_15M_INFO_V6 }}
          FILE_HIST_IP_V6: ${{ env.FILE_HIST_IP_V6 }}
          FILE_HIST_INFO_V6: ${{ env.FILE_HIST_INFO_V6 }}
          FILE_LATENCY: ${{ env.FILE_LATENCY }}
          FILE_COLO: ${{ env.FILE_COLO }}
          FILE_LOG: ${{ env.FILE_LOG }}
        run: |
          cat << 'EOF' > run_script.py
          import requests
          from bs4 import BeautifulSoup
          import re
          import os
          from datetime import datetime, timedelta, timezone

          # --- 环境变量 ---
          F_15M_IP_V4 = os.getenv('FILE_15M_IP_V4', 'v4_15m_ip.md')
          F_15M_INFO_V4 = os.getenv('FILE_15M_INFO_V4', 'v4_15m_info.md')
          F_HIST_IP_V4 = os.getenv('FILE_HIST_IP_V4', 'v4_hist_ip.md')
          F_HIST_INFO_V4 = os.getenv('FILE_HIST_INFO_V4', 'v4_hist_info.md')
          
          F_15M_IP_V6 = os.getenv('FILE_15M_IP_V6', 'v6_15m_ip.md')
          F_15M_INFO_V6 = os.getenv('FILE_15M_INFO_V6', 'v6_15m_info.md')
          F_HIST_IP_V6 = os.getenv('FILE_HIST_IP_V6', 'v6_hist_ip.md')
          F_HIST_INFO_V6 = os.getenv('FILE_HIST_INFO_V6', 'v6_hist_info.md')

          F_LATENCY = os.getenv('FILE_LATENCY', 'latency.md')
          F_COLO = os.getenv('FILE_COLO', 'colo.md')
          F_LOG = os.getenv('FILE_LOG', 'log.md')

          MAX_HISTORY_COUNT = 30 # 每个分类历史保留数量
          MAX_LOG_LINES = 30
          HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

          # --- 辅助函数 ---
          def extract_int(s):
              """提取字符串中的第一个整数"""
              if not s: return 0
              match = re.search(r'(d+)', s)
              return int(match.group(1)) if match else 999999

          def parse_speed(s):
              """解析速度字符串为 kB/s 数值"""
              if not s: return 0
              s = s.lower()
              match = re.search(r'([d.]+)', s)
              if not match: return 0
              val = float(match.group(1))
              if 'gb' in s: val *= 1048576
              elif 'mb' in s: val *= 1024
              return int(val)

          def get_isp_category(isp_name):
              """标准化运营商名称"""
              if "移动" in isp_name: return "移动"
              if "联通" in isp_name: return "联通"
              if "电信" in isp_name: return "电信"
              return "其他"

          # --- Markdown 写入辅助 ---
          def write_categorized_tables(filename, grouped_data, headers, title_prefix=""):
              """
              grouped_data: 字典 {'移动': [rows], '联通': [rows]...}
              """
              order = ["移动", "联通", "电信", "其他"]
              with open(filename, 'w', encoding='utf-8') as f:
                  f.write(f"# {title_prefix} (分类统计)\n\n")
                  for isp in order:
                      rows = grouped_data.get(isp, [])
                      if not rows: continue
                      
                      f.write(f"### {isp} ({len(rows)}个)\n")
                      # 表头
                      f.write("| " + " | ".join(headers) + " |\n")
                      f.write("| " + " | ".join(['---'] * len(headers)) + " |\n")
                      # 内容
                      for row in rows:
                          row_str = [str(x) if x else " " for x in row]
                          f.write("| " + " | ".join(row_str) + " |\n")
                      f.write("\n")

          def write_categorized_codeblocks(filename, grouped_nodes, title_prefix=""):
              """纯IP文件也按分类写入代码块"""
              order = ["移动", "联通", "电信", "其他"]
              with open(filename, 'w', encoding='utf-8') as f:
                  f.write(f"# {title_prefix} (纯IP)\n\n")
                  for isp in order:
                      nodes = grouped_nodes.get(isp, [])
                      if not nodes: continue
                      f.write(f"### {isp}\n")
                      f.write("```text\n")
                      for node in nodes:
                          f.write(f"{node.ip}\n")
                      f.write("```\n\n")

          # --- 类定义：测速节点 ---
          class IpNode:
              def __init__(self, ip, isp, speed_str, bandwidth, latency, datacenter, ip_type=4, raw_line=None):
                  self.ip = ip
                  self.isp = isp
                  self.category = get_isp_category(isp) # 自动分类
                  self.speed_str = speed_str
                  self.bandwidth = bandwidth
                  self.latency = latency
                  self.datacenter = datacenter
                  self.ip_type = ip_type # 4 or 6
                  self.speed_int = parse_speed(speed_str)
                  
                  # 历史数据兼容
                  if raw_line and self.speed_int == 0:
                      self.speed_int = parse_speed(raw_line) # 尝试从原始行提取

              def to_md_row(self):
                  return [f"`{self.ip}`", self.isp, self.speed_str, self.bandwidth, self.latency, self.datacenter]

          # --- 1. 抓取测速数据 (v4 和 v6) ---
          def fetch_speed_data():
              # (Name, URL, Type)
              urls = [
                  ("EdgeOne-v4", "https://www.wetest.vip/page/edgeone/address_v4.html", 4),
                  ("CloudFlare-v4", "https://www.wetest.vip/page/cloudflare/address_v4.html", 4),
                  ("CloudFront-v4", "https://www.wetest.vip/page/cloudfront/address_v4.html", 4),
                  ("CloudFlare-v6", "https://www.wetest.vip/page/cloudflare/address_v6.html", 6),
                  ("CloudFront-v6", "https://www.wetest.vip/page/cloudfront/address_v6.html", 6)
              ]
              nodes_v4 = []
              nodes_v6 = []
              
              print("抓取测速数据...")
              for name, url, ip_type in urls:
                  try:
                      resp = requests.get(url, headers=HEADERS, timeout=15)
                      if resp.status_code != 200: continue
                      soup = BeautifulSoup(resp.text, 'html.parser')
                      for tr in soup.find_all('tr'):
                          tds = tr.find_all('td')
                          if len(tds) < 6: continue
                          try:
                              isp = tds[0].get_text(strip=True)
                              ip = tds[1].get_text(strip=True)
                              bw = tds[2].get_text(strip=True)
                              spd = tds[3].get_text(strip=True)
                              lat = tds[4].get_text(strip=True)
                              dc = tds[5].get_text(strip=True)
                              
                              # 简单的有效性检查
                              valid = False
                              if ip_type == 4 and ip.count('.') == 3: valid = True
                              if ip_type == 6 and ':' in ip: valid = True
                              
                              if valid:
                                  node = IpNode(ip, isp, spd, bw, lat, dc, ip_type)
                                  if node.speed_int > 0:
                                      if ip_type == 4:
                                          nodes_v4.append(node)
                                      else:
                                          nodes_v6.append(node)
                          except: continue
                  except Exception as e:
                      print(f"抓取错误 {name}: {e}")
              return nodes_v4, nodes_v6

          # --- 2. 抓取并分类延迟数据 (仅 IPv4) ---
          def fetch_latency_categorized():
              url = "https://www.wetest.vip/page/cloudflare/total_v4.html"
              # 结构: {'移动': [{'ip':x, 'lat':50, 'desc':'50ms'}], ...}
              grouped = {'移动': [], '联通': [], '电信': []}
              
              print("抓取延迟数据...")
              try:
                  resp = requests.get(url, headers=HEADERS, timeout=15)
                  if resp.status_code == 200:
                      soup = BeautifulSoup(resp.text, 'html.parser')
                      # 网页结构：IP | 移动延迟 | 联通延迟 | 电信延迟
                      for tr in soup.find_all('tr'):
                          tds = tr.find_all('td')
                          if len(tds) < 4: continue
                          
                          ip = tds[0].get_text(strip=True)
                          if ip.count('.') != 3: continue
                          
                          # 提取三网数据
                          cm_str = tds[1].get_text(strip=True) # 移动
                          cu_str = tds[2].get_text(strip=True) # 联通
                          ct_str = tds[3].get_text(strip=True) # 电信
                          
                          # 放入各自的列表 (为了单独排序)
                          # 只有当延迟有效时才加入
                          cm_val = extract_int(cm_str)
                          cu_val = extract_int(cu_str)
                          ct_val = extract_int(ct_str)

                          # 构建统一的行数据: [IP, 延迟详情]
                          if cm_val < 900: 
                              grouped['移动'].append({'val': cm_val, 'row': [f"`{ip}`", cm_str]})
                          if cu_val < 900:
                              grouped['联通'].append({'val': cu_val, 'row': [f"`{ip}`", cu_str]})
                          if ct_val < 900:
                              grouped['电信'].append({'val': ct_val, 'row': [f"`{ip}`", ct_str]})
              except Exception as e:
                  print(f"延迟数据抓取失败: {e}")
              
              # 排序逻辑：按延迟升序 (val 越小越好)
              result_rows = {}
              for isp in grouped:
                  # 排序
                  grouped[isp].sort(key=lambda x: x['val'])
                  # 取前15个，并提取 row
                  top_items = grouped[isp][:15]
                  result_rows[isp] = [item['row'] for item in top_items]
                  
              return result_rows

          # --- 3. 抓取数据中心 ---
          def fetch_colo_data():
              url = "https://www.wetest.vip/page/cloudflare/colo.html"
              rows = []
              try:
                  resp = requests.get(url, headers=HEADERS, timeout=15)
                  if resp.status_code == 200:
                      soup = BeautifulSoup(resp.text, 'html.parser')
                      for tr in soup.find_all('tr'):
                          tds = tr.find_all('td')
                          if len(tds) < 8: continue
                          rows.append([
                              tds[0].get_text(strip=True),
                              tds[1].get_text(strip=True),
                              tds[2].get_text(strip=True),
                              tds[3].get_text(strip=True),
                              f"`{tds[4].get_text(strip=True)}`",
                              f"`{tds[5].get_text(strip=True)}`",
                              f"`{tds[6].get_text(strip=True)}`",
                              tds[7].get_text(strip=True)
                          ])
              except: pass
              return rows

          # --- 历史文件解析 ---
          def load_history_from_md(file_path, ip_type=4):
              nodes = []
              if not os.path.exists(file_path): return nodes
              try:
                  with open(file_path, 'r', encoding='utf-8') as f:
                      for line in f:
                          if "|" not in line or "---" in line or "IP" in line: continue
                          parts = [p.strip() for p in line.split('|') if p.strip()]
                          if len(parts) >= 6:
                              ip = parts[0].replace('`', '')
                              nodes.append(IpNode(ip, parts[1], parts[2], parts[3], parts[4], parts[5], ip_type))
              except: pass
              return nodes

          def write_single_table(filename, headers, rows):
              with open(filename, 'w', encoding='utf-8') as f:
                  f.write("| " + " | ".join(headers) + " |\n")
                  f.write("| " + " | ".join(['---'] * len(headers)) + " |\n")
                  for r in rows:
                      f.write("| " + " | ".join(r) + " |\n")

          def update_log(msg):
              lines = []
              if os.path.exists(F_LOG):
                  with open(F_LOG, 'r', encoding='utf-8') as f: lines = f.readlines()
              lines.insert(0, f"- {msg}\n")
              with open(F_LOG, 'w', encoding='utf-8') as f: f.writelines(lines[:MAX_LOG_LINES])

          # --- 通用处理流程 (V4 或 V6) ---
          def process_nodes_group(current_nodes, hist_file_info, f_15m_info, f_15m_ip, f_hist_info, f_hist_ip, ip_type_name="IPv4"):
              # 1. 15分钟数据写入
              group_15m_nodes = {'移动': [], '联通': [], '电信': [], '其他': []}
              group_15m_rows = {'移动': [], '联通': [], '电信': [], '其他': []}
              
              for node in current_nodes:
                  group_15m_nodes[node.category].append(node)
                  
              for cat in group_15m_nodes:
                  group_15m_nodes[cat].sort(key=lambda x: x.speed_int, reverse=True)
                  group_15m_rows[cat] = [n.to_md_row() for n in group_15m_nodes[cat]]
                  
              write_categorized_tables(f_15m_info, group_15m_rows, ["IP", "线路", "速度", "带宽", "延迟", "数据中心"], f"{ip_type_name} 15分钟内测速结果")
              write_categorized_codeblocks(f_15m_ip, group_15m_nodes, f"{ip_type_name} 15分钟内纯IP")
              
              # 2. 历史数据处理
              hist_nodes = load_history_from_md(hist_file_info)
              
              # 合并去重
              ip_map = {}
              for n in hist_nodes:
                  if n.ip not in ip_map or n.speed_int > ip_map[n.ip].speed_int:
                      ip_map[n.ip] = n
              for n in current_nodes:
                  if n.ip not in ip_map or n.speed_int > ip_map[n.ip].speed_int:
                      ip_map[n.ip] = n
                      
              # 重新分类
              group_hist_nodes = {'移动': [], '联通': [], '电信': [], '其他': []}
              group_hist_rows = {'移动': [], '联通': [], '电信': [], '其他': []}
              
              for n in ip_map.values():
                  group_hist_nodes[n.category].append(n)
                  
              # 排序截取
              for cat in group_hist_nodes:
                  group_hist_nodes[cat].sort(key=lambda x: x.speed_int, reverse=True)
                  group_hist_nodes[cat] = group_hist_nodes[cat][:MAX_HISTORY_COUNT]
                  group_hist_rows[cat] = [n.to_md_row() for n in group_hist_nodes[cat]]
                  
              write_categorized_tables(f_hist_info, group_hist_rows, ["IP", "线路", "速度", "带宽", "延迟", "数据中心"], f"{ip_type_name} 历史最快 Top30")
              write_categorized_codeblocks(f_hist_ip, group_hist_nodes, f"{ip_type_name} 历史最快纯IP")

          # ================= 主流程 =================
          if __name__ == "__main__":
              utc_now = datetime.now(timezone.utc)
              time_str = (utc_now + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M:%S')

              # 1. 获取所有数据
              current_v4, current_v6 = fetch_speed_data()
              
              # 2. 处理 IPv4
              process_nodes_group(current_v4, F_HIST_INFO_V4, F_15M_INFO_V4, F_15M_IP_V4, F_HIST_INFO_V4, F_HIST_IP_V4, "IPv4")
              
              # 3. 处理 IPv6
              process_nodes_group(current_v6, F_HIST_INFO_V6, F_15M_INFO_V6, F_15M_IP_V6, F_HIST_INFO_V6, F_HIST_IP_V6, "IPv6")

              # 4. 处理延迟统计 (仅 IPv4)
              latency_grouped_rows = fetch_latency_categorized()
              write_categorized_tables(F_LATENCY, latency_grouped_rows, ["IP地址", "延迟详情"], "24小时三网延迟最低 (升序)")

              # 5. 数据中心
              colo_rows = fetch_colo_data()
              write_single_table(F_COLO, ["代码", "地区", "国家", "洲", "移动IP", "联通IP", "电信IP", "更新时间"], colo_rows)

              # 6. 日志
              v4_count = len(current_v4)
              v6_count = len(current_v6)
              best_v4 = current_v4[0].ip if current_v4 else "无"
              update_log(f"**{time_str}** 更新 | IPv4数:{v4_count} (最快:{best_v4}) | IPv6数:{v6_count} | 延迟统计已更新")
              print("任务完成")

          EOF
          python run_script.py

      - name: Commit and Push
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add *.md
          if git diff --staged --quiet; then
            echo "无变更"
          else
            git commit -m "Update Categorized IPs: $(date "+%Y-%m-%d %H:%M:%S")"
            git push
          fi
