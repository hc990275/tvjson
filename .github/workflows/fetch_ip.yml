name: Update IP and Time (BJ)

on:
  schedule:
    # GitHub 免费版限制最小间隔约 5 分钟
    - cron: '*/15 * * * *'
  workflow_dispatch:

# =========================================================
# 核心配置区域
# =========================================================
env:
  # --- 1. 速度测试 (15分钟/历史) ---
  FILE_15M_IP: "15分钟内_纯IP.md"
  FILE_15M_INFO: "15分钟内_完整信息.md"
  FILE_HIST_IP: "历史最快_纯IP.md"
  FILE_HIST_INFO: "历史最快_完整信息.md"
   
  # --- 2. 延迟与数据中心 ---
  FILE_LATENCY: "24小时三网延迟最低统计.md"
  FILE_COLO: "CloudFlare数据中心.md"
   
  # --- 3. 日志与汇总 ---
  FILE_LOG: "运行日志.md"
  FILE_YXIP: "yxip.txt"  # <--- [新增] 本次运行抓取到的所有IP纯文本

# =========================================================

permissions:
  contents: write

jobs:
  fetch-and-update:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Dependencies
        run: pip install requests beautifulsoup4

      - name: Run Script
        # 传递环境变量
        env:
          FILE_15M_IP: ${{ env.FILE_15M_IP }}
          FILE_15M_INFO: ${{ env.FILE_15M_INFO }}
          FILE_HIST_IP: ${{ env.FILE_HIST_IP }}
          FILE_HIST_INFO: ${{ env.FILE_HIST_INFO }}
          FILE_LATENCY: ${{ env.FILE_LATENCY }}
          FILE_COLO: ${{ env.FILE_COLO }}
          FILE_LOG: ${{ env.FILE_LOG }}
          FILE_YXIP: ${{ env.FILE_YXIP }}
        run: |
          cat << 'EOF' > run_script.py
          import requests
          from bs4 import BeautifulSoup
          import re
          import os
          from datetime import datetime, timedelta, timezone

          # --- 环境变量 ---
          F_15M_IP = os.getenv('FILE_15M_IP', '15m_ip.md')
          F_15M_INFO = os.getenv('FILE_15M_INFO', '15m_info.md')
          F_HIST_IP = os.getenv('FILE_HIST_IP', 'hist_ip.md')
          F_HIST_INFO = os.getenv('FILE_HIST_INFO', 'hist_info.md')
          F_LATENCY = os.getenv('FILE_LATENCY', 'latency.md')
          F_COLO = os.getenv('FILE_COLO', 'colo.md')
          F_LOG = os.getenv('FILE_LOG', 'log.md')
          F_YXIP = os.getenv('FILE_YXIP', 'yxip.txt')

          MAX_HISTORY_COUNT = 30 # 每个分类历史保留数量
          MAX_LOG_LINES = 30
          HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

          # --- 辅助函数 ---
          def extract_int(s):
              if not s: return 0
              match = re.search(r'(\d+)', s)
              return int(match.group(1)) if match else 999999

          def get_isp_category(isp_name):
              if "移动" in isp_name: return "移动"
              if "联通" in isp_name: return "联通"
              if "电信" in isp_name: return "电信"
              return "其他"

          # --- Markdown 写入辅助 ---
          def write_categorized_tables(filename, grouped_data, headers, title_prefix=""):
              order = ["移动", "联通", "电信", "其他"]
              with open(filename, 'w', encoding='utf-8') as f:
                  f.write(f"# {title_prefix} (分类统计)\n\n")
                  for isp in order:
                      rows = grouped_data.get(isp, [])
                      if not rows: continue
                      f.write(f"### {isp} ({len(rows)}个)\n")
                      f.write("| " + " | ".join(headers) + " |\n")
                      f.write("| " + " | ".join(['---'] * len(headers)) + " |\n")
                      for row in rows:
                          row_str = [str(x) if x else " " for x in row]
                          f.write("| " + " | ".join(row_str) + " |\n")
                      f.write("\n")

          def write_categorized_codeblocks(filename, grouped_nodes, title_prefix=""):
              order = ["移动", "联通", "电信", "其他"]
              with open(filename, 'w', encoding='utf-8') as f:
                  f.write(f"# {title_prefix} (纯IP)\n\n")
                  for isp in order:
                      nodes = grouped_nodes.get(isp, [])
                      if not nodes: continue
                      f.write(f"### {isp}\n")
                      f.write("```text\n")
                      for node in nodes:
                          f.write(f"{node.ip}\n")
                      f.write("```\n\n")

          class IpNode:
              def __init__(self, ip, isp, speed_str, bandwidth, latency, datacenter, raw_line=None):
                  self.ip = ip
                  self.isp = isp
                  self.category = get_isp_category(isp)
                  self.speed_str = speed_str
                  self.bandwidth = bandwidth
                  self.latency = latency
                  self.datacenter = datacenter
                  self.speed_int = extract_int(speed_str)
                  if raw_line and self.speed_int == 0:
                      self.speed_int = extract_int(raw_line)

              def to_md_row(self):
                  return [f"`{self.ip}`", self.isp, self.speed_str, self.bandwidth, self.latency, self.datacenter]

          # --- 1. 抓取测速数据 ---
          def fetch_speed_data():
              urls = [
                  ("EdgeOne", "https://www.wetest.vip/page/edgeone/address_v4.html"),
                  ("CloudFlare", "https://www.wetest.vip/page/cloudflare/address_v4.html")
              ]
              nodes = []
              print("抓取测速数据...")
              for name, url in urls:
                  try:
                      resp = requests.get(url, headers=HEADERS, timeout=15)
                      if resp.status_code != 200: continue
                      soup = BeautifulSoup(resp.text, 'html.parser')
                      for tr in soup.find_all('tr'):
                          tds = tr.find_all('td')
                          if len(tds) < 6: continue
                          try:
                              isp = tds[0].get_text(strip=True)
                              ip = tds[1].get_text(strip=True)
                              bw = tds[2].get_text(strip=True)
                              spd = tds[3].get_text(strip=True)
                              lat = tds[4].get_text(strip=True)
                              dc = tds[5].get_text(strip=True)
                              if ip.count('.') == 3:
                                  node = IpNode(ip, isp, spd, bw, lat, dc)
                                  if node.speed_int > 0:
                                      nodes.append(node)
                          except: continue
                  except Exception as e:
                      print(f"抓取错误 {name}: {e}")
              return nodes

          # --- 2. 抓取并分类延迟数据 ---
          def fetch_latency_categorized():
              url = "https://www.wetest.vip/page/cloudflare/total_v4.html"
              grouped = {'移动': [], '联通': [], '电信': []}
              print("抓取延迟数据...")
              try:
                  resp = requests.get(url, headers=HEADERS, timeout=15)
                  if resp.status_code == 200:
                      soup = BeautifulSoup(resp.text, 'html.parser')
                      for tr in soup.find_all('tr'):
                          tds = tr.find_all('td')
                          if len(tds) < 4: continue
                          ip = tds[0].get_text(strip=True)
                          if ip.count('.') != 3: continue
                          cm_str = tds[1].get_text(strip=True)
                          cu_str = tds[2].get_text(strip=True)
                          ct_str = tds[3].get_text(strip=True)
                          cm_val = extract_int(cm_str)
                          cu_val = extract_int(cu_str)
                          ct_val = extract_int(ct_str)

                          if cm_val < 900: 
                              grouped['移动'].append({'val': cm_val, 'row': [f"`{ip}`", cm_str]})
                          if cu_val < 900:
                              grouped['联通'].append({'val': cu_val, 'row': [f"`{ip}`", cu_str]})
                          if ct_val < 900:
                              grouped['电信'].append({'val': ct_val, 'row': [f"`{ip}`", ct_str]})
              except Exception as e:
                  print(f"延迟数据抓取失败: {e}")
              
              result_rows = {}
              for isp in grouped:
                  grouped[isp].sort(key=lambda x: x['val'])
                  top_items = grouped[isp][:15]
                  result_rows[isp] = [item['row'] for item in top_items]
              return result_rows

          # --- 3. 抓取数据中心 ---
          def fetch_colo_data():
              url = "https://www.wetest.vip/page/cloudflare/colo.html"
              rows = []
              try:
                  resp = requests.get(url, headers=HEADERS, timeout=15)
                  if resp.status_code == 200:
                      soup = BeautifulSoup(resp.text, 'html.parser')
                      for tr in soup.find_all('tr'):
                          tds = tr.find_all('td')
                          if len(tds) < 8: continue
                          rows.append([
                              tds[0].get_text(strip=True),
                              tds[1].get_text(strip=True),
                              tds[2].get_text(strip=True),
                              tds[3].get_text(strip=True),
                              f"`{tds[4].get_text(strip=True)}`",
                              f"`{tds[5].get_text(strip=True)}`",
                              f"`{tds[6].get_text(strip=True)}`",
                              tds[7].get_text(strip=True)
                          ])
              except: pass
              return rows

          def load_history_from_md():
              nodes = []
              if not os.path.exists(F_HIST_INFO): return nodes
              try:
                  with open(F_HIST_INFO, 'r', encoding='utf-8') as f:
                      for line in f:
                          if "|" not in line or "---" in line or "IP" in line: continue
                          parts = [p.strip() for p in line.split('|') if p.strip()]
                          if len(parts) >= 6:
                              ip = parts[0].replace('`', '')
                              nodes.append(IpNode(ip, parts[1], parts[2], parts[3], parts[4], parts[5]))
              except: pass
              return nodes

          def write_single_table(filename, headers, rows):
              with open(filename, 'w', encoding='utf-8') as f:
                  f.write("| " + " | ".join(headers) + " |\n")
                  f.write("| " + " | ".join(['---'] * len(headers)) + " |\n")
                  for r in rows:
                      f.write("| " + " | ".join(r) + " |\n")

          def update_log(msg):
              lines = []
              if os.path.exists(F_LOG):
                  with open(F_LOG, 'r', encoding='utf-8') as f: lines = f.readlines()
              lines.insert(0, f"- {msg}\n")
              with open(F_LOG, 'w', encoding='utf-8') as f: f.writelines(lines[:MAX_LOG_LINES])

          # ================= 主流程 =================
          if __name__ == "__main__":
              utc_now = datetime.now(timezone.utc)
              time_str = (utc_now + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M:%S')

              # 1. 获取当前测速 (本次运行)
              current_nodes = fetch_speed_data()
              
              group_15m_nodes = {'移动': [], '联通': [], '电信': [], '其他': []}
              group_15m_rows = {'移动': [], '联通': [], '电信': [], '其他': []}
              for node in current_nodes:
                  group_15m_nodes[node.category].append(node)
              for cat in group_15m_nodes:
                  group_15m_nodes[cat].sort(key=lambda x: x.speed_int, reverse=True)
                  group_15m_rows[cat] = [n.to_md_row() for n in group_15m_nodes[cat]]

              write_categorized_tables(F_15M_INFO, group_15m_rows, ["IP", "线路", "速度", "带宽", "延迟", "数据中心"], "15分钟内测速结果")
              write_categorized_codeblocks(F_15M_IP, group_15m_nodes, "15分钟内纯IP")

              # 2. 处理历史数据
              hist_nodes = load_history_from_md()
              ip_map = {}
              for n in hist_nodes:
                  if n.ip not in ip_map or n.speed_int > ip_map[n.ip].speed_int:
                      ip_map[n.ip] = n
              for n in current_nodes:
                  if n.ip not in ip_map or n.speed_int > ip_map[n.ip].speed_int:
                      ip_map[n.ip] = n
              
              group_hist_nodes = {'移动': [], '联通': [], '电信': [], '其他': []}
              group_hist_rows = {'移动': [], '联通': [], '电信': [], '其他': []}
              for n in ip_map.values():
                  group_hist_nodes[n.category].append(n)
              for cat in group_hist_nodes:
                  group_hist_nodes[cat].sort(key=lambda x: x.speed_int, reverse=True)
                  group_hist_nodes[cat] = group_hist_nodes[cat][:MAX_HISTORY_COUNT]
                  group_hist_rows[cat] = [n.to_md_row() for n in group_hist_nodes[cat]]

              write_categorized_tables(F_HIST_INFO, group_hist_rows, ["IP", "线路", "速度", "带宽", "延迟", "数据中心"], "历史最快 Top30")
              write_categorized_codeblocks(F_HIST_IP, group_hist_nodes, "历史最快纯IP")

              # 3. 处理延迟统计 (本次运行)
              latency_grouped_rows = fetch_latency_categorized()
              write_categorized_tables(F_LATENCY, latency_grouped_rows, ["IP地址", "延迟详情"], "24小时三网延迟最低 (升序)")

              # 4. 数据中心
              colo_rows = fetch_colo_data()
              write_single_table(F_COLO, ["代码", "地区", "国家", "洲", "移动IP", "联通IP", "电信IP", "更新时间"], colo_rows)

              # 5. [修改] 仅收集“本次运行”发现的IP并去重写入 yxip.txt
              current_unique_ips = set()
              
              # 来源A: 本次测速结果 (current_nodes)
              for node in current_nodes:
                  current_unique_ips.add(node.ip)
              
              # 来源B: 本次延迟测试结果 (latency_grouped_rows)
              # 注意：latency_grouped_rows 里面只包含Top15，如果你需要所有扫描到的延迟IP，
              # 需要改 fetch_latency_categorized 返回所有，但目前网页结构限制只能拿到Top，
              # 这里我们收集所有出现在结果中的IP
              for isp, rows in latency_grouped_rows.items():
                  for row in rows:
                      raw_ip = row[0].replace('`', '').strip() # 去掉 Markdown 符号
                      if raw_ip:
                          current_unique_ips.add(raw_ip)
              
              # 覆盖写入 (只保存本次)
              try:
                  with open(F_YXIP, 'w', encoding='utf-8') as f:
                      for ip in sorted(list(current_unique_ips)):
                          f.write(f"{ip}\n")
              except Exception as e:
                  print(f"写入YXIP失败: {e}")

              # 6. 日志
              count_now = len(current_nodes)
              count_yxip = len(current_unique_ips)
              best_now = current_nodes[0].ip if current_nodes else "无"
              update_log(f"**{time_str}** 更新 | 测速IP:{count_now} | 本次抓取去重IP:{count_yxip} | 最快:{best_now}")
              print("任务完成")

          EOF
          python run_script.py

      - name: Commit and Push
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add *.md *.txt
          if git diff --staged --quiet; then
            echo "无变更"
          else
            git commit -m "Update IPs: $(date "+%Y-%m-%d %H:%M:%S")"
            git push
          fi
